# Voice-OS: Voice-Activated macOS Assistant

A modular, voice-activated OS assistant for macOS that uses LLM (Claude) for intent understanding and AppleScript for system control.

## Features

- **üîä Voice/Text Input**: Real macOS speech recognition or text input
- **ü§ñ LLM Intent Parsing**: Uses Anthropic Claude API to understand natural language commands
- **üîó Multi-Step Task Chaining**: Execute multiple commands in sequence (NEW!)
- **üîÑ Rule-Based Fallback**: Automatic fallback to keyword-based rules when LLM fails
- **‚úÖ Strict Schema Validation**: All LLM outputs validated by Pydantic
- **üñ•Ô∏è macOS Integration**: Execute commands via AppleScript and shell
- **üõ°Ô∏è Safety First**: Dangerous commands require confirmation
- **üß™ Dry-Run Mode**: Test without executing commands
- **üîä TTS Feedback**: Speaks results using macOS `say` command

## Supported Intents

1. **system_setting**: Adjust volume, brightness, etc.
2. **play_music**: Control music playback
3. **web_search**: Search the web (opens in Safari)
4. **write_note**: Create notes in Notes app
5. **control_app**: Open/control applications
6. **clarify**: Request clarification for ambiguous/unsafe commands

## üîó Multi-Step Task Chaining (NEW!)

Execute multiple commands in sequence with a single request:

```bash
# Two-step: Open app then search
python app/main.py run --text "ÊâìÂºÄSafariÁÑ∂ÂêéÊêúÁ¥¢PythonÊïôÁ®ã"

# Three-step: Search, note, and adjust volume
python app/main.py run --text "ÊêúÁ¥¢Â§©Ê∞îÔºåËÆ∞ÂΩï‰ªäÂ§©ÂøÉÊÉÖ‰∏çÈîôÔºåÊääÈü≥ÈáèË∞ÉÂà∞50%"

# Preview plan without executing
python app/main.py run --text "ÊâìÂºÄSafariÁÑ∂ÂêéÊêúÁ¥¢Python" --plan-debug
```

**How it works:**
- LLM detects multi-step keywords (ÁÑ∂Âêé, Êé•ÁùÄ, then, next, etc.)
- Generates a Plan with multiple Intent steps
- Executes sequentially, stops on first failure
- Safety confirmation for dangerous operations

**See [MULTI_STEP.md](MULTI_STEP.md) for complete documentation.**

## Project Structure

```
voice-os/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py          # CLI entry point (Typer)
‚îÇ   ‚îú‚îÄ‚îÄ config.py        # Configuration from .env
‚îÇ   ‚îú‚îÄ‚îÄ schema.py        # Pydantic intent models
‚îÇ   ‚îú‚îÄ‚îÄ planner.py       # LLM + rule-based orchestration
‚îÇ   ‚îú‚îÄ‚îÄ llm.py           # Anthropic API client
‚îÇ   ‚îú‚îÄ‚îÄ asr.py           # ASR placeholder (text input)
‚îÇ   ‚îú‚îÄ‚îÄ tts.py           # macOS 'say' wrapper
‚îÇ   ‚îú‚îÄ‚îÄ verbalizer.py    # Generate natural responses
‚îÇ   ‚îú‚îÄ‚îÄ executor.py      # Execute intents on macOS
‚îÇ   ‚îî‚îÄ‚îÄ utils.py         # AppleScript helpers
‚îú‚îÄ‚îÄ executor/
‚îÇ   ‚îî‚îÄ‚îÄ macos/
‚îÇ       ‚îú‚îÄ‚îÄ system.applescript   # Volume control
‚îÇ       ‚îú‚îÄ‚îÄ safari.applescript   # Open URLs
‚îÇ       ‚îî‚îÄ‚îÄ notes.applescript    # Create notes
‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îú‚îÄ‚îÄ system.txt       # LLM system prompt
‚îÇ   ‚îî‚îÄ‚îÄ fewshot.jsonl    # Few-shot examples
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ tasks.csv        # Test cases (30 utterances)
‚îÇ   ‚îî‚îÄ‚îÄ replay.py        # Accuracy evaluation script
‚îú‚îÄ‚îÄ .env.example         # Environment template
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

## Quick Start

### 1. Prerequisites

- macOS (tested on macOS 10.15+)
- Python 3.11+
- Anthropic API key

### 2. Installation

```bash
# Clone or navigate to project directory
cd voice-os

# Install dependencies
pip install -r requirements.txt

# Copy environment template
cp .env.example .env

# Edit .env and add your Anthropic API key
# ANTHROPIC_API_KEY=sk-ant-xxxxx
```

### 3. Run Examples

```bash
# Single command (with LLM)
python app/main.py --text "ÊääÈü≥ÈáèË∞ÉÂà∞30%"

# Dry-run mode (show what would be executed)
python app/main.py --text "ÊêúÁ¥¢PythonÊïôÁ®ã" --dry-run

# Rule-based only (no LLM, no API key needed)
python app/main.py --text "Êí≠ÊîæÈü≥‰πê" --no-llm

# Interactive loop mode
python app/main.py

# Run basic tests
python app/main.py test

# Run accuracy evaluation (rule-based)
python tests/replay.py

# Run accuracy evaluation (with LLM, requires API key)
python tests/replay.py --llm
```

## Example Inputs & Expected Results

### Example 1: Volume Control
```
Input:  "ÊääÈü≥ÈáèË∞ÉÂà∞30%"
Intent: system_setting
Slots:  {"setting": "volume", "value": 30}
Action: Executes system.applescript to set volume to 30%
Output: "ËÆæÁΩÆÂ∑≤ÂÆåÊàê" (spoken via TTS)
```

### Example 2: Web Search
```
Input:  "ÊêúÁ¥¢PythonÊïôÁ®ã"
Intent: web_search
Slots:  {"query": "PythonÊïôÁ®ã"}
Action: Opens Safari with Google search for "PythonÊïôÁ®ã"
Output: "Â∑≤ÊâìÂºÄÊêúÁ¥¢ÁªìÊûú" (spoken via TTS)
```

### Example 3: Dangerous Command (requires confirmation)
```
Input:  "Âà†Èô§ÊâÄÊúâÊñá‰ª∂"
Intent: clarify
Confirm: true
Safety: {"risk": "high", "reason": "Destructive operation"}
Output: "ÊÇ®Á°ÆÂÆöË¶ÅÊâßË°å„ÄåÂà†Èô§ÊâÄÊúâÊñá‰ª∂„ÄçÂêóÔºüËøôÂèØËÉΩÊúâÈ£éÈô©„ÄÇ" (asks for confirmation)
```

## CLI Options

```bash
python app/main.py run [OPTIONS]

Options:
  --text, -t TEXT    Direct text input (skip ASR)
  --dry-run          Only show what would be executed
  --plan-debug       Show plan without executing (for multi-step)
  --no-llm           Use rule-based only (no API calls)
  --loop, -l         Continuous listening mode
  --help             Show help message
```

## Development

### Adding New Intents

1. Update `IntentName` in `app/schema.py`
2. Add extraction logic in `app/planner.py` (rule-based)
3. Add execution logic in `app/executor.py`
4. Update prompts in `prompts/system.txt` and `prompts/fewshot.jsonl`
5. Add test cases in `tests/tasks.csv`

### Adding New AppleScripts

1. Create `.applescript` file in `executor/macos/`
2. Follow pattern: `on run argv ... end run`
3. Update `app/executor.py` to call the script

## Testing

```bash
# Test single-step intents with provided CSV tasks (rule-based, ~70% accuracy expected)
python tests/replay.py

# Test single-step with LLM (~90% accuracy expected)
python tests/replay.py --llm

# Test multi-step planning (NEW!)
python tests/plan_replay.py

# Add your own test cases
# Single-step: Edit tests/tasks.csv
# Multi-step: Edit tests/plan_tasks.csv
```

## Safety Features

- **Keyword Detection**: Dangerous keywords (delete, format, shutdown) trigger safety checks
- **Confirmation Required**: High-risk operations require explicit user confirmation
- **Dry-Run Mode**: Test commands without executing
- **Error Recovery**: Failed LLM calls automatically fall back to rule-based parsing

## Limitations & Future Work

- **TTS**: Uses basic `say` command; could improve with better voice/speed control
- **Intents**: Currently 6 intents; can extend to more macOS automation (file operations, notifications, etc.)
- **Context**: No conversation history or context tracking
- **Cross-step dependencies**: Multi-step tasks execute independently, no data passing between steps yet

## Troubleshooting

### "ANTHROPIC_API_KEY not set"
- Copy `.env.example` to `.env` and add your API key
- Or use `--no-llm` flag to run without LLM

### AppleScript Permission Errors
- macOS may require accessibility permissions for certain operations
- Go to System Preferences > Security & Privacy > Accessibility
- Add Terminal or your Python executable

### TTS Not Working
- Ensure macOS `say` command is available: `say "test"`
- Check audio output is not muted

## License

MIT License - feel free to use and modify for your needs.

## Contributing

This is a learning project and starter template. Feel free to:
- Report issues
- Suggest improvements
- Add new intents/features
- Improve LLM prompts

---

Built with ‚ù§Ô∏è for macOS automation enthusiasts.
